{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e6d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b4fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc8579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "def load_json_data(filepath: str) -> list:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            report_data = json.load(f)\n",
    "\n",
    "        if report_data:\n",
    "            if isinstance(report_data, list):\n",
    "                print(f\"Successfully loaded JSON. Found {len(report_data)} records.\")\n",
    "            else:\n",
    "                print(f\"Loaded JSON, but it's not a list. Type: {type(report_data)}. Handling appropriately.\")\n",
    "                report_data = [report_data] if isinstance(report_data, dict) else []\n",
    "        else:\n",
    "            print(\"WARNING: JSON file was loaded but appears to be empty or None.\")\n",
    "            report_data = []\n",
    "        return report_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at '{filepath}'. Please check the path.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"ERROR: Could not decode JSON from file {filepath}. The file may be corrupted. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred during file loading: {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db207b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON. Found 8634 records.\n",
      "Successfully loaded JSON. Found 8634 records.\n",
      "Successfully loaded JSON. Found 8634 records.\n",
      "Successfully loaded JSON. Found 7023 records.\n",
      "Successfully loaded JSON. Found 7023 records.\n",
      "Successfully loaded JSON. Found 7023 records.\n"
     ]
    }
   ],
   "source": [
    "# --- ACTIONABILITY & SPECIFICITY ---\n",
    "# Load the master DataFrame for actionability and specificity\n",
    "master_df = pd.DataFrame(load_json_data('dataset/preprocessed/master_df.json'))\n",
    "actionability_df = pd.DataFrame(load_json_data('dataset/evaluations/actionability_results.json'))\n",
    "specificity_df = pd.DataFrame(load_json_data('dataset/evaluations/specificity_results.json'))\n",
    "\n",
    "\n",
    "# --- RUBRIC ALIGNMENT & FACTIONAL CONSISTENCY ---\n",
    "# Load the master DataFrame for rubric alignment and factual consistency\n",
    "master_second_df = pd.DataFrame(load_json_data('dataset/preprocessed/master_second_df.json'))\n",
    "rubric_alignment_df = pd.DataFrame(load_json_data('dataset/evaluations/rubric_alignment_results.json'))\n",
    "factual_consistency_df = pd.DataFrame(load_json_data('dataset/evaluations/factual_consistency_results.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a25ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8634 entries, 0 to 8633\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   assignment_id  8634 non-null   int64  \n",
      " 1   paper_content  8634 non-null   object \n",
      " 2   item           8634 non-null   object \n",
      " 3   type           8634 non-null   object \n",
      " 4   grader_score   8634 non-null   float64\n",
      " 5   comment        8634 non-null   object \n",
      " 6   evidence       7023 non-null   object \n",
      " 7   score_range    7023 non-null   object \n",
      " 8   rubric         7023 non-null   object \n",
      " 9   metric         8634 non-null   object \n",
      " 10  judge_score    8634 non-null   float64\n",
      " 11  status         8634 non-null   object \n",
      " 12  reason         8634 non-null   object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 877.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8634 entries, 0 to 8633\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   assignment_id  8634 non-null   int64  \n",
      " 1   paper_content  8634 non-null   object \n",
      " 2   item           8634 non-null   object \n",
      " 3   type           8634 non-null   object \n",
      " 4   grader_score   8634 non-null   float64\n",
      " 5   comment        8634 non-null   object \n",
      " 6   evidence       7023 non-null   object \n",
      " 7   score_range    7023 non-null   object \n",
      " 8   rubric         7023 non-null   object \n",
      " 9   metric         8634 non-null   object \n",
      " 10  judge_score    8634 non-null   float64\n",
      " 11  status         8634 non-null   object \n",
      " 12  reason         8634 non-null   object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 877.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7023 entries, 0 to 7022\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   assignment_id  7023 non-null   int64  \n",
      " 1   paper_content  7023 non-null   object \n",
      " 2   item           7023 non-null   object \n",
      " 3   type           7023 non-null   object \n",
      " 4   grader_score   7023 non-null   float64\n",
      " 5   comment        7023 non-null   object \n",
      " 6   evidence       7023 non-null   object \n",
      " 7   score_range    7023 non-null   object \n",
      " 8   rubric         7023 non-null   object \n",
      " 9   metric         7023 non-null   object \n",
      " 10  judge_score    7023 non-null   float64\n",
      " 11  status         7023 non-null   object \n",
      " 12  reason         7023 non-null   object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 713.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7023 entries, 0 to 7022\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   assignment_id  7023 non-null   int64  \n",
      " 1   paper_content  7023 non-null   object \n",
      " 2   item           7023 non-null   object \n",
      " 3   type           7023 non-null   object \n",
      " 4   grader_score   7023 non-null   float64\n",
      " 5   comment        7023 non-null   object \n",
      " 6   evidence       7023 non-null   object \n",
      " 7   score_range    7023 non-null   object \n",
      " 8   rubric         7023 non-null   object \n",
      " 9   metric         7023 non-null   object \n",
      " 10  judge_score    7023 non-null   float64\n",
      " 11  status         7023 non-null   object \n",
      " 12  reason         7023 non-null   object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 713.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare the DataFrames for Merging ---\n",
    "# Ensure both DataFrames have a 'paper_content' column for merging.\n",
    "# Rename 'input' to 'paper_content' for clarity.\n",
    "actionability_df.rename(columns={'actual_output': 'comment'}, inplace=True)\n",
    "specificity_df.rename(columns={'actual_output': 'comment'}, inplace=True)\n",
    "rubric_alignment_df.rename(columns={'actual_output': 'comment'}, inplace=True)\n",
    "factual_consistency_df.rename(columns={'input': 'comment'}, inplace=True)\n",
    "\n",
    "\n",
    "# Now, merge the two DataFrames by finding rows with the exact same 'comment' text.\n",
    "# This is a foolproof way to align the rows correctly, regardless of their original order.\n",
    "merged_actionability_df = pd.merge(actionability_df, master_df, on='comment', how='left')\n",
    "merged_specificity_df = pd.merge(specificity_df, master_df, on='comment', how='left')\n",
    "merged_rubric_alignment_df = pd.merge(rubric_alignment_df, master_second_df, on='comment', how='left')\n",
    "merged_factual_consistency_df = pd.merge(factual_consistency_df, master_second_df, on='comment', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# --- Display the Final, Correctly Merged DataFrame ---\n",
    "# Sort the merged DataFrames by 'assignment_id' and 'item' for better readability.\n",
    "# This will ensure that the rows are in a consistent order.\n",
    "merged_actionability_df.sort_values(['assignment_id','item'], inplace=True)\n",
    "merged_actionability_df.reset_index(drop=True, inplace=True)\n",
    "merged_actionability_df.rename(columns={'score_x': 'judge_score'}, inplace=True)\n",
    "merged_actionability_df.rename(columns={'score_y': 'grader_score'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_specificity_df.sort_values(['assignment_id','item'], inplace=True)\n",
    "merged_specificity_df.reset_index(drop=True, inplace=True)\n",
    "merged_specificity_df.rename(columns={'score_x': 'judge_score'}, inplace=True)\n",
    "merged_specificity_df.rename(columns={'score_y': 'grader_score'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_rubric_alignment_df.sort_values(['assignment_id','item'], inplace=True)\n",
    "merged_rubric_alignment_df.reset_index(drop=True, inplace=True)\n",
    "merged_rubric_alignment_df.rename(columns={'score_x': 'judge_score'}, inplace=True)\n",
    "merged_rubric_alignment_df.rename(columns={'score_y': 'grader_score'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_factual_consistency_df.sort_values(['assignment_id','item'], inplace=True)\n",
    "merged_factual_consistency_df.reset_index(drop=True, inplace=True)\n",
    "merged_factual_consistency_df.rename(columns={'score_x': 'judge_score'}, inplace=True)\n",
    "merged_factual_consistency_df.rename(columns={'score_y': 'grader_score'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- Final Cleanup of Columns ---\n",
    "# Keep only the necessary columns for the final DataFrame.\n",
    "final_columns_to_keep = [\n",
    "    'assignment_id',\n",
    "    'paper_content',\n",
    "    'item',\n",
    "    'type', \n",
    "    'grader_score',\n",
    "    'comment',\n",
    "    'evidence',\n",
    "    'score_range',\n",
    "    'rubric',\n",
    "    'metric',\n",
    "    'judge_score',\n",
    "    'status',\n",
    "    'reason'\n",
    "    ]\n",
    "\n",
    "\n",
    "# Filter the DataFrames to keep only the final columns.\n",
    "# This ensures that we have a consistent structure across all merged DataFrames.\n",
    "merged_actionability_df = merged_actionability_df[final_columns_to_keep]\n",
    "merged_specificity_df = merged_specificity_df[final_columns_to_keep]\n",
    "merged_rubric_alignment_df = merged_rubric_alignment_df[final_columns_to_keep]\n",
    "merged_factual_consistency_df = merged_factual_consistency_df[final_columns_to_keep]\n",
    "\n",
    "\n",
    "# --- Save the Final Merged DataFrames ---\n",
    "merged_actionability_df.to_json('dataset/merged/merged_actionability.json', orient='records', indent=4)\n",
    "merged_specificity_df.to_json('dataset/merged/merged_specificity.json', orient='records', indent=4)\n",
    "merged_rubric_alignment_df.to_json('dataset/merged/merged_rubric_alignment.json', orient='records', indent=4)\n",
    "merged_factual_consistency_df.to_json('dataset/merged/merged_factual_consistency.json', orient='records', indent=4)\n",
    "\n",
    "\n",
    "merged_actionability_df.info()\n",
    "merged_specificity_df.info()\n",
    "merged_rubric_alignment_df.info()\n",
    "merged_factual_consistency_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keath_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
