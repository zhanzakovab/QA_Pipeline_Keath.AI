# AI Quality Assurance Pipeline for Automated Feedback  
*Scalable LLM evaluation framework built in collaboration with KEATH.AI*  

## Overview  
This project delivers a **production-ready AI Quality Assurance (QA) pipeline** for evaluating **AI-generated student feedback** at scale.  
Developed with **KEATH.AI**, an EdTech company building automated grading systems, the pipeline ensures that feedback generated by **Grader LLMs** is **actionable, specific, rubric-aligned, and factually consistent** — bridging the gap between **AI capabilities** and **educational trust**.  

> **Goal:** Automate quality control for LLM-generated feedback and provide **data-driven insights** into system performance and bias.

---

## Key Features  
- **Large-scale evaluation** → Processed **8,600+ AI-generated feedback items** across multiple assignments  
- **Multi-metric QA** → Evaluates four core dimensions of pedagogical quality:  
  `Actionability · Specificity · Rubric Alignment · Factual Consistency`
- **Judge vs. Grader LLM setup** → Uses **secondary LLMs** to evaluate outputs from **primary LLMs**  
- **Failure mode analytics** → Detects systemic weaknesses, bias patterns, and student-level inequities  
- **Extensible architecture** → Designed to plug into other **multi-LLM QA pipelines** in EdTech or beyond  

---

## Tech Stack & Architecture  
- **Languages**: `Python`  
- **Frameworks & Libraries**:  
  - **LLM Evaluation** → `deepeval`, `GEval`, `LLM-as-a-Judge`  
  - **Data Processing** → `pandas`, `numpy`  
  - **Visualization** → `matplotlib`, `seaborn`, custom color palettes  
- **Scalable Data Pipelines**:  
  - Cleaned, merged, and analyzed ~**8.6k+ feedback items** efficiently  
  - Modular structure for easy adaptation to new datasets  
- **Analytics Outputs**:  
  - **Pass/fail heatmaps**  
  - **Correlation matrices** between evaluation metrics  
  - **Bias detection visualizations**  

---

## Example Insights  
- Identified that **lower-scoring students** systematically receive **lower-quality feedback** → potential **equity risk**  
- Quantified **metric correlations** (e.g. Actionability ↔ Specificity ≈ 0.31) to inform prompt optimization  
- Flagged **failure hotspots** in **Total Score rows** caused by missing rubric and evidence fields  

---

## Why It Matters  
AI-driven grading and feedback systems are scaling fast, but **trust depends on quality**.  
This project demonstrates a **practical, reproducible solution** for:  
- Automating **QA for LLM-generated content**  
- Surfacing **systemic biases** and **failure patterns**  
- Providing **actionable insights** to improve prompt design, data quality, and model selection  

---

## Repository Highlights  
- `evaluation_pipeline/` → Modular evaluation framework  
- `data_processing/` → Cleaning, merging, and transformations  
- `visualizations/` → Plots, heatmaps, and analytics outputs  
- `notebooks/` → Interactive analysis and insights  

---

## Future Extensions  
- Integrate **human-in-the-loop validation** for disputed evaluations  
- Deploy pipeline as a **REST API** for live AI QA  
- Expand to **multi-model benchmarking** for KEATH.AI's next-gen systems  

---

### Takeaway  
This is **not just an academic exercise** — it's a **scalable, production-ready QA framework** for **evaluating LLM systems** in real-world EdTech contexts.  
It shows end-to-end skills in:  
- **LLM evaluation**  
- **data engineering**  
- **analytics & visualization**  
- **bias detection & prompt optimization**  

---

**Author:** Botakoz Zhanzakova  
MSc Business Analytics, UCL  
